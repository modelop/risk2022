{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install ydata-quality\n",
    "!pip install -i https://modelop:opendatagroup@pypi.modelop.center/modelop/staging modelop==0.13.2\n",
    "!pip install nest_asyncio\n",
    "!pip install boto3\n",
    "!pip install minio\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Restart the kernel now"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelop Automated Test Developer\n",
    "\n",
    "## Provide ModelOp Center and S3 Configuration\n",
    "\n",
    "In order to read information from ModelOp Center, we must provide the location of where ModelOp Center is installed and\n",
    "our S3 access credentials.  Please run the cell and fill them below and click the Update button."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display, HTML\n",
    "import os\n",
    "import html\n",
    "import requests\n",
    "from ipywidgets import widgets, HBox, VBox, Label, Output, Select\n",
    "from ipywidgets import HTML as widgetHTML\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "moc_url = \"http://modelop.k7jeo.com/\"\n",
    "#moc_url = \"http://mocaasin.modelop.center/\"\n",
    "os.environ[\"modelop.gateway-url\"] = moc_url\n",
    "config_out = widgets.Output(layout={'border': '1px solid black'})\n",
    "s3_access_key = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "s3_secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "\n",
    "\n",
    "def update_values(button):\n",
    "\tglobal moc_url, s3_access_key, s3_secret_key\n",
    "\tmoc_url = moc_url_box.value\n",
    "\ts3_access_key = s3_access_key_box.value\n",
    "\ts3_secret_key = s3_secret_key_box.value\n",
    "\twith config_out:\n",
    "\t\tprint(\"Using modelop center url: \" + moc_url)\n",
    "\t\tprint(\"S3 Keys Updated\")\n",
    "\n",
    "\n",
    "display(widgets.Label('ModelOp Center URL'))\n",
    "moc_url_box = widgets.Text(value=moc_url)\n",
    "display(moc_url_box)\n",
    "display(widgets.Label('S3 Access Key'))\n",
    "s3_access_key_box = widgets.Password(value=s3_access_key)\n",
    "display(s3_access_key_box)\n",
    "display(widgets.Label('S3 Secret Key'))\n",
    "s3_secret_key_box = widgets.Password(value=s3_secret_key)\n",
    "display(s3_secret_key_box)\n",
    "update_button = widgets.Button(description='Update Values')\n",
    "update_button.on_click(update_values)\n",
    "display(update_button)\n",
    "display(config_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test for Authentication Requirements and Authenticate\n",
    "\n",
    "Test to see if the target ModelOp Center instance requires authentication.  If authentication is required we will be prompted for authentication information and should login utilizing our oauth credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "require.undef('sign_in_widget');\n",
    "\n",
    "function getJupyterUrl () {\n",
    "    return location.protocol+'//' + location.hostname + (location.port ? ':'+location.port: '');\n",
    "}\n",
    "\n",
    "function removeEndingSlash( url ) {\n",
    "    return url.replace(/\\/$/, \"\");\n",
    "}\n",
    "\n",
    "function randomString(length) {\n",
    "    var chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890,./;'[]\\=-)(*&^%$#@!~`\";\n",
    "    var result = \"\";\n",
    "    for (var i = length; i > 0; --i) result += chars[Math.floor(Math.random() * chars.length)];\n",
    "    return result;\n",
    "}\n",
    "\n",
    "define('sign_in_widget', [\"@jupyter-widgets/base\"], function(widgets) {\n",
    "\n",
    "    var SignInView = widgets.DOMWidgetView.extend({\n",
    "\n",
    "        getOAuth2Token: function( widget, customPagePath, finalBaseUrl ) {\n",
    "            finalBaseUrl = removeEndingSlash(finalBaseUrl)\n",
    "            $.ajax({\n",
    "                type: 'GET',\n",
    "                url: finalBaseUrl + '/api/oauth2/.well-known-configuration/jupyter',\n",
    "                success: function( response ) {\n",
    "                    widget.redirectToLogin(widget, customPagePath, finalBaseUrl, response);\n",
    "                },\n",
    "                error: function( jqXHR, textStatus, error ) {\n",
    "                    if (jqXHR.status == 404) {\n",
    "                        widget.login_result.textContent = \"Login not required (or no well known configuration found for jupyter)\";\n",
    "                        widget.model.save_changes();\n",
    "                    } else {\n",
    "                        console.log(\"An error occurred while trying to get the authorization uri from gateway service.\");\n",
    "                        widget.login_result.textContent = \"Couldn't connect with the provided Url: \" + finalBaseUrl;\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "        },\n",
    "\n",
    "        redirectToLogin: function (widget, customPagePath, finalBaseUrl, oAuth2Data ) {\n",
    "            let formattedScope = oAuth2Data.scopes.join(\" \");\n",
    "\n",
    "            var redirectUriParams = {\n",
    "                client_id: oAuth2Data.clientId,\n",
    "                response_type: oAuth2Data.responseType,\n",
    "                scope: formattedScope,\n",
    "                state: getJupyterUrl() + customPagePath,\n",
    "                nonce: randomString(10),\n",
    "                redirect_uri: oAuth2Data.redirectUri\n",
    "            };\n",
    "            widget.displayClientAuthorization(widget, oAuth2Data.oAuth2Provider.authorizationUri + \"?\" + $.param(redirectUriParams));\n",
    "        },\n",
    "\n",
    "        displayClientAuthorization: function (widget, url ) {\n",
    "            var w = 500;\n",
    "            var h = 600;\n",
    "            const y = window.top.outerHeight / 2 + window.top.screenY - ( h / 2);\n",
    "            const x = window.top.outerWidth / 2 + window.top.screenX - ( w / 2);\n",
    "            var login_popup = window.open(url, 'Client Authorization', `toolbar=no, location=no, directories=no, status=no, menubar=no, scrollbars=no, resizable=no, copyhistory=no, width=${w}, height=${h}, top=${y}, left=${x}`);\n",
    "            var timer = setInterval(function() {\n",
    "                try {\n",
    "                    if(login_popup.closed) {\n",
    "                        clearInterval(timer);\n",
    "                        if (!widget.token_input.value) {\n",
    "                            widget.login_result.textContent = \"Login interrupted!\"\n",
    "                        }\n",
    "                    }\n",
    "                    try {\n",
    "                        if (login_popup.location?.hash) {\n",
    "                            let parsedHash = new URLSearchParams(login_popup.location.hash.substring(1));\n",
    "                            widget.model.set('token', parsedHash.get(\"access_token\"));\n",
    "                            widget.login_result.textContent = \"Login successful!\"\n",
    "                            widget.model.save_changes();\n",
    "                            login_popup.close()\n",
    "                        }\n",
    "                    } catch {}\n",
    "                } catch {\n",
    "                    clearInterval(timer);\n",
    "                }\n",
    "            }, 200);\n",
    "        },\n",
    "\n",
    "        // Render the view.\n",
    "        render: function() {\n",
    "            this.getOAuth2Token(this, \"/\", this.model.get('base_url'))\n",
    "            this.login_result = document.createElement('div');\n",
    "            this.token_input = document.createElement('hidden');\n",
    "            this.token_input.type = 'text';\n",
    "            this.token_input.value = this.model.get('token');\n",
    "            this.token_input.disabled = this.model.get('disabled');\n",
    "\n",
    "            this.el.appendChild(this.login_result);\n",
    "            this.el.appendChild(this.token_input);\n",
    "\n",
    "            // Python -> JavaScript update\n",
    "            this.model.on('change:token', this.value_changed, this);\n",
    "            this.model.on('change:disabled', this.disabled_changed, this);\n",
    "\n",
    "            // JavaScript -> Python update\n",
    "            this.token_input.onchange = this.input_changed.bind(this);\n",
    "        },\n",
    "\n",
    "        value_changed: function() {\n",
    "            this.token_input.value = this.model.get('token');\n",
    "        },\n",
    "\n",
    "        disabled_changed: function() {\n",
    "            this.token_input.disabled = this.model.get('disabled');\n",
    "        },\n",
    "\n",
    "        input_changed: function() {\n",
    "            this.model.set('token', this.token_input.value);\n",
    "            this.model.save_changes();\n",
    "        },\n",
    "    });\n",
    "\n",
    "    return {\n",
    "        SignInView: SignInView\n",
    "    };\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from traitlets import Unicode, Bool\n",
    "from ipywidgets import DOMWidget, register\n",
    "\n",
    "\n",
    "@register\n",
    "class SignInWidget(DOMWidget):\n",
    "\t_view_name = Unicode('SignInView').tag(sync=True)\n",
    "\t_view_module = Unicode('sign_in_widget').tag(sync=True)\n",
    "\t_view_module_version = Unicode('0.1.0').tag(sync=True)\n",
    "\n",
    "\t# Attributes\n",
    "\ttoken = Unicode(\"\", help=\"The oauth2 token obtained.\").tag(sync=True)\n",
    "\tdisabled = Bool(True, help=\"Enable or disable user changes.\").tag(sync=True)\n",
    "\tbase_url = Unicode(\"http://modelop.k7jeo.com\", help=\"The base URL\").tag(sync=True)\n",
    "\n",
    "\n",
    "login = SignInWidget(base_url=moc_url)\n",
    "display(login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Choose Target Model\n",
    "\n",
    "A standardized test model runs against a given target model.  It will pull the required data resources, such as baseline and/or comparator data, from this target model.  So in developing your dashboard or standardized test, we will start with picking an example target model that is currently registered with ModelOp Center to run our dashboard or test against.  This model should already have baseline and comparator data defined as assets stored in S3.  Run the cell and select one of the models in the drop down box before proceeding to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({'Authorization': f'Bearer {login.token}'})\n",
    "try:\n",
    "\tresponse = session.get(moc_url + 'model-manage/api/storedModelSummaries?size=500')\n",
    "\n",
    "\tavailable_models = []\n",
    "\tresponse.raise_for_status()\n",
    "\tif response.ok:\n",
    "\t\tmodel_list = response.json()\n",
    "\t\tfor model in model_list[\"_embedded\"][\"storedModelSummaries\"]:\n",
    "\t\t\tavailable_models.append((model[\"modelMetaData\"][\"name\"], model[\"id\"]))\n",
    "\tmodel_selector = widgets.Dropdown(options=available_models, description='Target Model:')\n",
    "\tdisplay(model_selector)\n",
    "except Exception as e:\n",
    "\tdisplay(HTML('Couldn\\'t connect to ' + moc_url + '</br><pre><code>' + str(e) + '</code></pre>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Target Model and Select DMN\n",
    "\n",
    "By running the cell below, we will contact model manage and fetch the definition of the example target model.\n",
    "This information will be used for running your dashboard model against an existing model definition.  You will be presented with a list of available dmn files that we can use to analyze the results of the tests.  This can be created from scratch in the latter part of this notebook if none currently exists, but this can give you a starting place to provide common values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_dmn(button) :\n",
    "    global dmn\n",
    "    global dmn_selector\n",
    "    global dmn_asset\n",
    "    dmn_asset = dmn_selector.value\n",
    "    dmn = dmn_selector.value.get('sourceCode', \"\")\n",
    "    display(HTML(\"<pre><div id='dmn' dmnxml='\" + html.escape(dmn) + \"'>\"+ html.escape(dmn) + \"</div></pre>\"))    \n",
    "    \n",
    "    \n",
    "model = {}\n",
    "dmn = \"\"\n",
    "response = session.get(moc_url + 'model-manage/api/storedModels/' + model_selector.value)\n",
    "if response.ok:\n",
    "    model = response.json()\n",
    "    display(VBox([\n",
    "        HBox([Label(value=\"Model Name:\"), Label(model.get('modelMetaData', {}).get('name', 'Name not set'))]),\n",
    "        HBox([Label(value=\"Description:\"), Label(model.get('modelMetaData', '').get('description', ''))]),\n",
    "        HBox([Label(value=\"Created By:\"), Label(model.get('createdBy', 'Unknown'))]),\n",
    "        HBox([Label(value=\"Created Date:\"), Label(model.get('createdDate', 'Unknown'))])\n",
    "    ]))\n",
    "    display(HTML(\"Model loaded and can be viewed <a href=\" + moc_url + \"#/models/model/\" + model[\n",
    "        \"id\"] + \" target=\\\"_blank\\\" >here</a>.\"))\n",
    "    available_dmn_assets = []\n",
    "    for asset in model['modelAssets']:\n",
    "        if asset['assetRole'] == 'TEST_RESULT_COMPARATOR':\n",
    "            available_dmn_assets.append((asset.get('name', 'unknown'), asset))\n",
    "    dmn_selector = widgets.Dropdown(options=available_dmn_assets, description= 'DMN Asset')\n",
    "    display(dmn_selector)\n",
    "    select_dmn_button = widgets.Button(description='Select DMN')\n",
    "    select_dmn_button.on_click(select_dmn)\n",
    "    display(select_dmn_button)    \n",
    "            \n",
    "else:\n",
    "    print(\n",
    "        \"ERROR - The model could not be found, please update the example_model_name variable with the correct model name to examine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Choose the Datasets To Run\n",
    "\n",
    "Select the existing asset for which we should use for a test run of the metrics function.  This would typically be the baseline data and comparator data.  For each dataset, select them in the order you wish to pass them to the metrics function, and push the add dataset button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "available_assets = []\n",
    "selected_assets = []\n",
    "selected_asset = {}\n",
    "sample_df = None\n",
    "datasets = []\n",
    "for asset in model[\"modelAssets\"]:\n",
    "    if asset[\"assetType\"] == 'EXTERNAL_FILE' or asset[\"assetType\"] == 'FILE':\n",
    "        available_assets.append((asset.get('filename', asset.get('name', 'unknown')) + ' (' + asset[\"assetRole\"] + ')', asset))\n",
    "        \n",
    "asset_selector = widgets.Dropdown(options=available_assets, description='Test Asset')\n",
    "\n",
    "def on_asset_selector_change(change):\n",
    "    global sample_df\n",
    "    global selected_asset\n",
    "    global data_display\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        selected_asset = asset_selector.value\n",
    "        if selected_asset[\"assetType\"] == 'EXTERNAL_FILE':\n",
    "            file = None\n",
    "            try:\n",
    "#                s3_client = Minio(selected_asset[\"repositoryInfo\"][\"host\"] + ':' + str(selected_asset[\"repositoryInfo\"][\"port\"]),\n",
    "                s3_client = Minio('modelop.k7jeo.com:' + str(selected_asset[\"repositoryInfo\"][\"port\"]),\n",
    "                                  access_key=s3_access_key,\n",
    "                                  secret_key=s3_secret_key,\n",
    "                                  secure=selected_asset[\"repositoryInfo\"][\"secure\"])\n",
    "                file = s3_client.get_object(selected_asset[\"repositoryInfo\"][\"host\"],\n",
    "                                           selected_asset[\"name\"])\n",
    "            except:\n",
    "                s3_client = boto3.client('s3', region_name='us-east-2',\n",
    "                                         aws_access_key_id=s3_access_key,\n",
    "                                         aws_secret_access_key=s3_secret_key)\n",
    "                bucket=selected_asset[\"repositoryInfo\"][\"host\"].split('.')[0]\n",
    "                file = s3_client.get_object(Bucket=bucket, Key=selected_asset[\"name\"])[\"Body\"].read()\n",
    "                \n",
    "            if selected_asset[\"name\"].endswith('.csv'):\n",
    "                sample_df = pd.read_csv(file)\n",
    "            elif selected_asset[\"name\"].endswith('.json'):\n",
    "                sample_df = pd.read_json(file, lines=True)\n",
    "        elif selected_asset[\"assetType\"] == 'FILE':\n",
    "                if selected_asset[\"name\"].endswith('.csv'):\n",
    "                    sample_df = pd.read_csv(selected_asset[\"fileContentString\"])\n",
    "                elif selected_asset[\"name\"].endswith('.json'):\n",
    "                    sample_df = pd.read_json(selected_asset[\"fileContentString\"], lines=True)\n",
    "        if sample_df is not None:\n",
    "            data_out = Output()\n",
    "            with data_out:\n",
    "                display(sample_df)\n",
    "            data_display.children = tuple([widgetHTML('Sample data file: ' + selected_asset[\"filename\"]), data_out])\n",
    "            \n",
    "def add_asset(button):\n",
    "    selected_assets.append(selected_asset)\n",
    "    datasets.append(sample_df)\n",
    "    selected_datasets.options = tuple(list(selected_datasets.options) + [ selected_asset.get('filename', selected_asset.get('name', 'unknown'))])\n",
    "                \n",
    "asset_selector.observe(on_asset_selector_change)\n",
    "display(asset_selector)\n",
    "add_asset_button = widgets.Button(description='Add Dataset')\n",
    "add_asset_button.on_click(add_asset)\n",
    "selected_datasets = widgets.Select(description= 'Datasets')\n",
    "display(selected_datasets)\n",
    "display(add_asset_button)\n",
    "data_display = widgets.VBox()\n",
    "display(data_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Initilization and Metrics Function\n",
    "\n",
    "Now we will implement a metrics function that constitutes the standardized test.  This metrics function can return any valid json output of nested name/value pairs.  This information, along with model metadata, will be used for evaluation by the DMN file later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import traceback\n",
    "import modelop.schema.infer as infer\n",
    "import modelop.monitors.performance as performance\n",
    "import modelop.monitors.drift as drift\n",
    "\n",
    "DEPLOPYABLE_MODEL = {}\n",
    "INPUT_SCHEMA = {}\n",
    "\n",
    "def init(init_param):\n",
    "    global DEPLOYABLE_MODEL\n",
    "    global INPUT_SCHEMA\n",
    "    \n",
    "    job = json.loads(init_param['rawJson'])\n",
    "    DEPLOYABLE_MODEL = job['referenceModel']\n",
    "    INPUT_SCHEMA = infer.extract_input_schema(init_param)\n",
    "    \n",
    "def metrics(baseline, comparator) -> dict:\n",
    "    global DEPLOYABLE_MODEL\n",
    "    global INPUT_SCHEMA\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    result.update(\n",
    "        {'modelUseCategory': DEPLOYABLE_MODEL.get('storedModel', {}).get('modelMetaData', {}).get('modelUseCategory', ''),\n",
    "        'modelOrganization': DEPLOYABLE_MODEL.get('storedModel', {}).get('modelMetaData', {}).get('modelOrganization', ''),\n",
    "        'modelRisk': DEPLOYABLE_MODEL.get('storedModel', {}).get('modelMetaData', {}).get('modelRisk', ''),\n",
    "        'modelMethodology': DEPLOYABLE_MODEL.get('storedModel', {}).get('modelMetaData', {}).get('modelMethodology', '')}\n",
    "                 )\n",
    "    result.update(calculate_performance(comparator, INPUT_SCHEMA))\n",
    "    \n",
    "    result.update(calculate_ks_drift(baseline, comparator))\n",
    "    \n",
    "    yield result\n",
    "    \n",
    "def calculate_performance(comparator, input_schema) -> dict:\n",
    "    try:\n",
    "        monitoring_parameters = infer.set_monitoring_parameters(input_schema, check_schema=True)\n",
    "        performance_test = performance.ModelEvaluator(dataframe=comparator,\n",
    "                                                        score_column=monitoring_parameters.get('score_column', None),\n",
    "                                                        label_column=monitoring_parameters.get('label_column', None))\n",
    "        if DEPLOYABLE_MODEL.get('storedModel', {}).get('modelMetaData', {}).get('modelMethodology', '').casefold() == 'regression'.casefold():\n",
    "            performance_result = performance_test.evaluate_performance(pre_defined_metrics='regression_metrics')\n",
    "            return {\n",
    "                'performance': [ performance_result ],\n",
    "                'mae': performance_result.get('values', {}).get('mae', None),\n",
    "                'r2_score': performance_result.get('values', {}).get('r2_score', None),\n",
    "                'rmse': performance_result.get('values', {}).get('rmse', None)\n",
    "            }\n",
    "        else:\n",
    "            performance_result = performance_test.evaluate_performance(pre_defined_metrics='classification_metrics')\n",
    "            return {\n",
    "                'performance': [ performance_result ],\n",
    "                'accuracy': performance_result.get('values', {}).get('accuracy', None),\n",
    "                'precision': performance_result.get('values', {}).get('precision', None),\n",
    "                'recall': performance_result.get('values', {}).get('recall', None),\n",
    "                'f1_score': performance_result.get('values', {}).get('f1_score', None),\n",
    "                'auc': performance_result.get('values', {}).get('auc', None),\n",
    "            }\n",
    "    except Exception as ex:\n",
    "        print('Error occurred calculating performance metrics')\n",
    "        print(ex)\n",
    "        print(traceback.format_exc())\n",
    "        return {}\n",
    "    \n",
    "def calculate_ks_drift(baseline, sample) -> dict:\n",
    "    try:\n",
    "        drift_test = drift.DriftDetector(df_baseline=baseline, df_sample=sample)\n",
    "        drift_result = drift_test.calculate_drift(pre_defined_test='Kolmogorov-Smirnov')\n",
    "        return {\"data_drift\": [ drift_result ]}\n",
    "    except:\n",
    "        print(\"Error occurred while calculating drift\")\n",
    "        print(ex)\n",
    "        print(traceback.format_exc())\n",
    "        return {}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Execute Metrics Function\n",
    "\n",
    "Take the chosen asset file and execute the metrics function utilizing the given data.  A stub job is created that simulates running the metrics as a batch job against the previously selected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "job = {'deployableModel': {'storedModel': model},\n",
    "       'inputData': selected_assets,\n",
    "       'referenceModel': {'storedModel': model}\n",
    "      }\n",
    "metrics_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "metrics_result = None\n",
    "with metrics_output:\n",
    "    init({'rawJson': json.dumps(job)})\n",
    "    gen_func = metrics(datasets[0], datasets[1])\n",
    "    first_result = next(gen_func)\n",
    "display(metrics_output)\n",
    "display(HTML('<div><pre><code>' + json.dumps(first_result, indent=3, sort_keys=True) + '</code></pre></div'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and Edit DMN File\n",
    "\n",
    "Here we will instantiate an instance of a DMN editor and load in the previously selected dmn file contents.  Make any changes you wish to the values or columns, and then hit the update dmn button before attempting test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/diagram-js.css\">\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/dmn-js-shared.css\">\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/dmn-js-drd.css\">\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/dmn-js-decision-table.css\">\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/dmn-js-decision-table-controls.css\">\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/dmn-js-literal-expression.css\">\n",
    "<link rel=\"stylesheet\" href=\"https://unpkg.com/dmn-js@12.1.0/dist/assets/dmn-font/css/dmn.css\">\n",
    "<div id=\"dmneditor\"></div>\n",
    "<button id=\"save-button\">Update DMN</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "\tpaths: {\n",
    "\t\tdmnmodeler: \"https://unpkg.com/dmn-js@12.1.0/dist/dmn-modeler.development\"\n",
    "\t}\n",
    "});\n",
    "require(['dmnmodeler'], function(dmn_modeler) {\n",
    "    console.log('Defining Editor');\n",
    "    var diagramUrl = 'https://cdn.staticaly.com/gh/bpmn-io/dmn-js-examples/a71e16/starter/diagram.dmn';\n",
    "    console.log(dmn_modeler)\n",
    "    const dmnModeler = new dmn_modeler( {\n",
    "\t\tcontainer: '#dmneditor'\n",
    "\t});\n",
    "      \n",
    "    function exportDiagram() {\n",
    "\n",
    "        dmnModeler.saveXML({ format: true }, function(err, xml) {\n",
    "\n",
    "            if (err) {\n",
    "                return console.error('could not save DMN 1.1 diagram', err);\n",
    "            }\n",
    "            \n",
    "            var updatedXmlCmd = \"updated_xml='\" + xml.replace(/\\r?\\n|\\r/g, \"\") + \"'\";\n",
    "            console.log(updatedXmlCmd)\n",
    "            IPython.notebook.kernel.execute(updatedXmlCmd);\n",
    "        });\n",
    "      }\n",
    "    \n",
    "    /**\n",
    "       * Open diagram in our modeler instance.\n",
    "       *\n",
    "       * @param {String} dmnXML diagram to display\n",
    "       */\n",
    "      function openDiagram(dmnXML) {\n",
    "\n",
    "        // import diagram\n",
    "        dmnModeler.importXML(dmnXML, function(err) {\n",
    "\n",
    "          if (err) {\n",
    "            return console.error('could not import DMN 1.1 diagram', err);\n",
    "          }\n",
    "          \n",
    "          // fetch currently active view\n",
    "          var activeView = dmnModeler.getActiveView();\n",
    "\n",
    "          // apply initial logic in DRD view\n",
    "          if (activeView.type === 'drd') {\n",
    "            var activeEditor = dmnModeler.getActiveViewer();\n",
    "\n",
    "            // access active editor components\n",
    "            var canvas = activeEditor.get('canvas');\n",
    "\n",
    "            // zoom to fit full viewport\n",
    "            canvas.zoom('fit-viewport');\n",
    "          }\n",
    "        });\n",
    "      }\n",
    "    \n",
    "    console.log(\"Getting Diagram\");\n",
    "    openDiagram(document.getElementById('dmn').getAttribute('dmnxml'));\n",
    "    $('#save-button').click(exportDiagram);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate Metrics Result with DMN\n",
    "\n",
    "When you run this cell, it will make a call to ModelOp Center to evaluate the given test results against the above updated DMN entry.  You will see the results of the evaluation.  Feel free to make changes to the DMN in the editor above and hit the update dmn button again, then return to this cell and execute to see the effects of these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dmn_asset[\"sourceCode\"]= updated_xml\n",
    "eval_request = {'dmnAsset': dmn_asset,\n",
    "                'variables': first_result\n",
    "               }\n",
    "result = session.post(url=moc_url + 'mlc-service/api/dmn/evaluate/', json=eval_request)\n",
    "print(json.dumps(result.json(), indent=3, sort_keys=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}